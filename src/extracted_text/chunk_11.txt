5.2 ScienceQA

ScienceQA [34] contains 21k multimodal multiple choice questions with rich domain diversity across 3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training, validation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two representative methods, including GPT-3.5 model (text-davinci-002) with and without chain- of-thought (CoT), LLaMA-Adapter [59], as well as multimodal chain-of-thought (MM-CoT) [61], which is the current SoTA method on this dataset. For more baseline numbers, please see [34].

The results are reported in Table 7. For LLaVA, we use the visual features before the last layer, ask the model to first predict reasons and then the answer, and train it for 12 epochs. It yields 90.92% accuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt GPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute gain compared with 75.17% from GPT-3.5. For a substantial number of questions, we note that GPT-4 fails simply because it reports that there is insufficient context such as images or plots. We consider two schemes to combine the outcomes from our model and GPT-4. (i) A GPT-4 complement. Whenever GPT-4 fails to provide answers, we use the prediction from our method. This schemes yields 90.97% accuracy, which is almost the same as applying our method alone. (ii) GPT-4 as the judge. Whenever GPT-4 and LLaVA produce different answers, we prompt GPT-4 again, asking it to provide its own final answer based on the question and two outcomes. The spirit is similar with CoT, but with the external knowledge from the other model. Surprisingly, this scheme is able to provide consistent improvement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly, the text-only GPT-4, which cannot process images, improves the overall performance of the model on questions that have an image as context. This is because some of these questions do not actually require the image context for a correct answer. The GPT-4 judge can identify such cases and correct some of the errors that LLaVA makes. See the example in Appendix. To the best of our knowledge,

8

Method NAT Subject SOC LAN Context Modality TXT IMG NO Grade G1-6 G7-12 Average Representative & SoTA methods with numbers reported in the literature Human [34] 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40 GPT-3.5 [34] 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97 GPT-3.5 w/ CoT [34] 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17 LLaMA-Adapter [59] 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19 MM-CoTBase [61] 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91 MM-CoTLarge [61] 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68 Results with our own experiment runs GPT-4† 84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69 LLaVA 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92 LLaVA+GPT-4† (complement) 90.36 95.50 88.55 89.05 87.80 91.08 92.22 88.73 90.97 LLaVA+GPT-4† (judge) 91.56 96.74 91.09 90.62 88.99 93.52 92.73 92.16 92.53

Table 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. †Text-only GPT-4, our eval. Our novel model ensembling with the text-only GPT-4 consistently improves the model’s performance under all categories, setting the new SoTA performance.

this is the first time that GPT-4 is used for model ensembling. We hope this finding can encourage future research to explore more effective methods to leverage LLMs for model ensembling.